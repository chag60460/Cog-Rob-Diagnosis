{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Problem Set: Diagnosis\n",
    "## Implicates and Implicants for Constraint Programming (50 points)\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Tree Method](#treemethod)\n",
    "3. [Prime implicate method](#prime-implicate)\n",
    "    1. [Detect conflicts](#conflict)\n",
    "    2. [Test validity](#validity)\n",
    "    3. [Test unsatisfiability](#unsatisfiable)\n",
    "    4. [Full algorithm](#full-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you load the dependencies below by highlighting the cell below and pressing Shift + Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from propositional_state_logic import *\n",
    "from sat_solver import *\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"introduction\"/>\n",
    "</a>\n",
    "\n",
    "In this problem set, you'll answer a theoretical question about the tree method and will also implement a prime implicate generator. This generator helps decrease the size of a model to make diagnosis more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Method (10 Points) <a id=\"treemethod\"/>\n",
    "</a>\n",
    "\n",
    "In this lecture we discussed Slagle's Tree Method (1970) which determines prime implicants through a depth-first search of an ordered tree in which the root \"holds\" all clauses in the theory to be processed. Describe the runtime of the tree method and provide justification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: The runtime of the tree method is $O(3^n)$.\n",
    "\n",
    "This is because the algorithm constructs a binary tree with a depth of n. SInce there are $2^n$ combinations of Boolean variable values, there are $2^n$ evaluatation required. However, each evaluation involves checking minterms, which can result in a total runtime of $O(3^n)$.\n",
    "\n",
    "GRADING NOTE: \n",
    "+ Full credit (10 points) will be given if the student says \"Exponential\" and provides some justification\n",
    "+ 5 points will be given if the student says \"Exponential\" and does not provide justification\n",
    "+ 5 points will be given if the student says a run time other than exponential, and provides justfication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prime implicate method <a id=\"prime-implicate\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this HW, we will use the thruster model described in the paper, \"Diagnosis: Implicates and Implicants for Constraint Programming\" and covered in lecture. We've also used tools from the Pset 3: Diagnosis, and included some additional ones to facilitate the implementation. First, we will define the model and constraints. Next, we will create stand alone methods to identify if a node is valid, if it is unsatisfiable, or if it is satisfiable. Based on these methods, we will write the full algorithm to generate prime implicates, based on the minimal conflicts that we find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$(runthruster = on) \\wedge (pressure = high) \\Rightarrow (thruster = thrust)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$(runthruster = on) \\wedge (pressure = low) \\Rightarrow (thruster = nothrust)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$(runthruster = off) \\Rightarrow (thruster = nothrust)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Problem()\n",
    "\n",
    "# Define the variables for the mini thruster problem with variables: T1, R1, and P3. Returns a Variable object.\n",
    "# Thrust: T1\n",
    "T1 = p.add_variable('thruster', type='finite_domain', domain=['thrust', 'nothrust'])\n",
    "# Thruster: R1\n",
    "R1 = p.add_variable('runthruster', type='finite_domain', domain=['on', 'off'])\n",
    "# Pressure before the thruster: P3\n",
    "P3 = p.add_variable('pressure', type='finite_domain', domain=['high', 'low'])\n",
    "\n",
    "# Add the theory / problem constraints.\n",
    "# The thruster only outputs thrust when it is on and when the input from P3 is high\n",
    "p.add_constraint('runthruster=on & pressure=high => thruster=thrust')\n",
    "p.add_constraint('runthruster=on & pressure=low => thruster=nothrust')\n",
    "p.add_constraint('runthruster=off => thruster=nothrust')\n",
    "\n",
    "# Prints out constraints nicely in LaTeX, so you can check them.\n",
    "display_constraints(p)\n",
    "\n",
    "# Define SAT for future use\n",
    "sat = SATSolver(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code, we create the structure of the tree over which we will iterate. In a true sceneario, we would not be searching over all of the tree (to save time), but for the purpose of this exercise, we iterate over all the tree to present the different statuses that a node can have: valid, unsatifiable, satisfiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thruster_model = {\n",
    "    frozenset([T1.get_assignment('thrust')]) : {\n",
    "        frozenset([T1.get_assignment('thrust'), R1.get_assignment('on')]) : {\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('on'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('on'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('thrust'), R1.get_assignment('off')]) : {\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('thrust'), P3.get_assignment('high')]) : {},\n",
    "        frozenset([T1.get_assignment('thrust'), P3.get_assignment('low')]) : {},\n",
    "        },\n",
    "    \n",
    "    frozenset([T1.get_assignment('nothrust')]) : {\n",
    "        frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on')]) : {\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off')]) : {\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('nothrust'), P3.get_assignment('high')]) : {},\n",
    "        frozenset([T1.get_assignment('nothrust'), P3.get_assignment('low')]) : {},\n",
    "        },\n",
    "    \n",
    "    frozenset([R1.get_assignment('on')]) : {\n",
    "            frozenset([R1.get_assignment('on'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([R1.get_assignment('on'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([R1.get_assignment('off')]) : {\n",
    "            frozenset([R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "    \n",
    "    frozenset([P3.get_assignment('high')]) : {},\n",
    "    \n",
    "    frozenset([P3.get_assignment('low')]) : {},\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect conflicts <a id=\"conflict\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the pruning rules that we have in our tree is to check for conflicts. Thanks to the helper function `check_consistency` that was provided to us for pset 3, it is easy to program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_conflict(sat, candidate):\n",
    "    return not sat.check_consistency(candidate)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check our function using assignments for which we know the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a conflict: This should return true\n",
    "# The thruster cannot thrust if the run thruster command is \"Off\"\n",
    "is_conflict(sat, frozenset([T1.get_assignment('thrust'), R1.get_assignment('off')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of an assignment that is not a conflict: This should return false\n",
    "# This is not a conflict because the thruster will thrust if the thruster command is \"On\" and the pressure before the thruster is \"High\"\n",
    "is_conflict(sat, frozenset([T1.get_assignment('thrust'), R1.get_assignment('on'), P3.get_assignment('high')]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test validity (10 Points) <a id=\"validity\"/>\n",
    "</a>\n",
    "\n",
    "For the second pruning rule, we need to identify if a candidate is valid. A candidate is valid if it is not a conflict and none of its children is conflict. We will use a helper function to get all of the children (keys) of a node, based on the structure of the tree we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def get_all_keys(d):\n",
    "    for key, value in d.items():\n",
    "        yield key\n",
    "        if isinstance(value, dict):\n",
    "            yield from get_all_keys(value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this helper function, we can check a candidate's validity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Write code for validity here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Test Validity\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Internal note: we will delete this when we hand it in to the students\n",
    "### ANSWER KEY ####\n",
    "def is_valid(candidate, sat, thruster_model_children):\n",
    "    # list of all the following partial assignments\n",
    "    childCandidateList = list(get_all_keys(thruster_model_children))\n",
    "    for child in childCandidateList:\n",
    "        if is_conflict(sat, child):\n",
    "            return False\n",
    "    return True\n",
    "### END OF ANSWER KEY ####\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a series of test on candidates that we know are valid or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### These will be NBgrader tests\n",
    "check_validity(is_valid)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validity Test 1: Candidate that is Not Valid\n",
    "candidate = frozenset([T1.get_assignment('nothrust')])\n",
    "thruster_model_subset_1 = {\n",
    "        frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on')]) : {\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off')]) : {\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('nothrust'), P3.get_assignment('high')]) : {},\n",
    "        frozenset([T1.get_assignment('nothrust'), P3.get_assignment('low')]) : {},\n",
    "        }\n",
    "\n",
    "is_valid(candidate, sat, thruster_model_subset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validity Test 2: Candidate that is Valid\n",
    "candidate = frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off')])\n",
    "thruster_model_subset = thruster_model_subset_1[candidate]\n",
    "\n",
    "is_valid(candidate, sat, thruster_model_subset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Unsatisfiability (10 Points) <a id=\"unsatisfiable\"/>\n",
    "</a>\n",
    "\n",
    "We now test for unsatisfiability. Similar to the test for validity, we need to check if a candidate is a conflict, and if all of it children are also conflicts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Write code for unsatisfiability here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Test for Unsatisfiability\n",
    "# NOTE: IF THE CANDIDATE ITSELF CONFLICTS WITH THE THEORY, THE CANDIDATE IS UNSATISFIABLE\n",
    "\n",
    "# WRITE CODE HERE\n",
    "\n",
    "# Internal note: we will delete this when we hand it in to the students\n",
    "### ANSWER KEY ####\n",
    "def is_unsatisfiable(candidate, sat, thruster_model_children):\n",
    "    # list of all the following partial assignments\n",
    "    childCandidateList = list(get_all_keys(thruster_model_children))\n",
    "    \n",
    "    if is_conflict(sat, candidate):\n",
    "        return True\n",
    "    \n",
    "    for child in childCandidateList:\n",
    "        if not is_conflict(sat, child): # if one of the children is consistent\n",
    "            return False\n",
    "    return True\n",
    "### END OF ANSWER KEY ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also run a series of test on candidates that we know are unsatisfiable or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### These will be NBgrader tests\n",
    "check_unsatisfiability(is_unsatisfiable)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsatisfiability Test 1: Candidate that is Unsatisfiable\n",
    "candidate = frozenset([T1.get_assignment('thrust'), R1.get_assignment('off')])\n",
    "thruster_model_subset_3 = {\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            }\n",
    "\n",
    "is_unsatisfiable(candidate, sat, thruster_model_subset_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsatisfiability Test 2: Candidate that is Not Unsatisfiable\n",
    "candidate = frozenset([T1.get_assignment('thrust')])\n",
    "thruster_model_subset_4 = {\n",
    "        frozenset([T1.get_assignment('thrust'), R1.get_assignment('on')]) : {\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('on'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('on'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('thrust'), R1.get_assignment('off')]) : {\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('high')]) : {},\n",
    "            frozenset([T1.get_assignment('thrust'), R1.get_assignment('off'), P3.get_assignment('low')]) : {},\n",
    "            },\n",
    "        frozenset([T1.get_assignment('thrust'), P3.get_assignment('high')]) : {},\n",
    "        frozenset([T1.get_assignment('thrust'), P3.get_assignment('low')]) : {},\n",
    "        }\n",
    "\n",
    "is_unsatisfiable(candidate, sat, thruster_model_subset_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full algorithm: Minimal Conflict Generator (20 Points) <a id=\"full-algorithm\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will iterate through each of the candidates received from the candidate generator, and determine if they should be either sent back to the generator, converted to an implicate, or added to the solutions list. Your `is_valid` and `is_unsatisfiable` functions from Questions 1 & 2 will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_implicate_finder(sat, candidateList):\n",
    "    ## sat represents the theory behind the represented model\n",
    "    ## CandidateList: List of Tuples (candidate, thruster_model_children), \n",
    "        # where the candidate is the item to be tested and \n",
    "        # where thruster_model_children is a dictionary holding the children of the given candidate\n",
    "    ## generatorAdditions: add candidate to generatorAdditions list when it should go to the generator\n",
    "    ## convertToImplicate: add candidate to the convertToImplicateList when it needs to be converted to an implicate before being passed to the generator\n",
    "    ## solutions: add candidate to the solutions list when a candidate is a minimal conflict\n",
    "    ### Function should return a tuple in the format (generatorAdditions, convertToImplicate, solutions)\n",
    "    \n",
    "    generatorAdditions = []\n",
    "    convertToImplicate = []\n",
    "    solutions = []\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    ### ANSWER KEY ###\n",
    "    for candidateTuple in candidateList:\n",
    "        candidate = candidateTuple[0]\n",
    "        thruster_model_children = candidateTuple[1]\n",
    "        \n",
    "        if is_valid(candidate, sat, thruster_model_children):\n",
    "            generatorAdditions.append(candidate)\n",
    "        elif is_unsatisfiable(candidate, sat, thruster_model_children):\n",
    "            solutions.append(candidate)\n",
    "        else: #candidate is satisfiable\n",
    "            convertToImplicate.append(candidate)\n",
    "    ### END ANSWER KEY ###        \n",
    "    return (generatorAdditions, convertToImplicate, solutions)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Problem\n",
    "\n",
    "candidate1 = frozenset([T1.get_assignment('thrust')])\n",
    "candidate2 = frozenset([T1.get_assignment('nothrust')])\n",
    "candidate4 = frozenset([R1.get_assignment('off')])\n",
    "candidate1A = frozenset([T1.get_assignment('thrust'), R1.get_assignment('on')])\n",
    "candidate1B = frozenset([T1.get_assignment('thrust'), R1.get_assignment('off')])\n",
    "candidate2A = frozenset([T1.get_assignment('nothrust'), R1.get_assignment('on')])\n",
    "candidate2B = frozenset([T1.get_assignment('nothrust'), R1.get_assignment('off')])\n",
    "\n",
    "\n",
    "candidateList = [\n",
    "    (candidate1, thruster_model[candidate1]),\n",
    "    (candidate2, thruster_model[candidate2]),\n",
    "    (candidate4, thruster_model[candidate4]),\n",
    "    (candidate1A,thruster_model[candidate1][candidate1A]),\n",
    "    (candidate1B,thruster_model[candidate1][candidate1B]),\n",
    "    (candidate2A,thruster_model[candidate2][candidate2A]),\n",
    "    (candidate2B,thruster_model[candidate2][candidate2B]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Additions:  [frozenset({(runthruster = off)}), frozenset({(thruster = nothrust), (runthruster = off)})] \n",
      "\n",
      "Convert To Implicate:  [frozenset({(thruster = thrust)}), frozenset({(thruster = nothrust)}), frozenset({(thruster = thrust), (runthruster = on)}), frozenset({(thruster = nothrust), (runthruster = on)})] \n",
      "\n",
      "Solutions:  [frozenset({(runthruster = off), (thruster = thrust)})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Prime Implicate Algorithm Output: \n",
    "\n",
    "(generatorAdditions, convertToImplicate, solutions) = prime_implicate_finder(sat, candidateList)\n",
    "print(\"Generator Additions: \", generatorAdditions, \"\\n\")\n",
    "print(\"Convert To Implicate: \", convertToImplicate, \"\\n\")\n",
    "print(\"Solutions: \", solutions, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\">\n",
       "        <strong>Tests passed!!</strong>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## NB Grader Tests\n",
    "check_prime_implicate_finder(prime_implicate_finder)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
